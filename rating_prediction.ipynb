{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 1: Rating Prediction via Prompting\n",
                "\n",
                "## Goal\n",
                "Predict 1â€“5 star ratings from Yelp review text using LLM prompting only (no ML model training).\n",
                "\n",
                "## Requirements\n",
                "- Dataset: Yelp Reviews (sample ~200)\n",
                "- Output: Strict JSON `{\"predicted_stars\": int, \"explanation\": str}`\n",
                "- Strategies: Zero-Shot, Reasoning, Few-Shot"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import random\n",
                "import pandas as pd\n",
                "from datasets import load_dataset\n",
                "from sklearn.metrics import accuracy_score\n",
                "import google.generativeai as genai\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Load API Key\n",
                "load_dotenv()\n",
                "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
                "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Dataset\n",
                "We use the `yelp_review_full` dataset from Hugging Face and select a sample of 200 reviews from the test split."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 200 samples.\n"
                    ]
                }
            ],
            "source": [
                "dataset = load_dataset(\"yelp_review_full\", split=\"test\")\n",
                "samples = dataset.select(range(200))\n",
                "print(f\"Loaded {len(samples)} samples.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Prompts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_gemini_response(prompt):\n",
                "    try:\n",
                "        response = model.generate_content(prompt)\n",
                "        text = response.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
                "        return json.loads(text)\n",
                "    except Exception:\n",
                "        return None\n",
                "\n",
                "def prompt_zero_shot(text):\n",
                "    return f\"\"\"\n",
                "    You are a sentiment analysis expert. Analyze the review and predict rating (1-5).\n",
                "    Review: \"{text}\"\n",
                "    Output JSON: {{\"predicted_stars\": <int>, \"explanation\": \"<str>\"}}\n",
                "    \"\"\"\n",
                "\n",
                "def prompt_reasoning(text):\n",
                "    return f\"\"\"\n",
                "    Analyze sentiment, tone, complaints, and praise. First determine if happy/neutral/angry.\n",
                "    Then decide rating 1-5.\n",
                "    Review: \"{text}\"\n",
                "    Output JSON: {{\"predicted_stars\": <int>, \"explanation\": \"<str>\"}}\n",
                "    \"\"\"\n",
                "\n",
                "def prompt_few_shot(text, examples):\n",
                "    ex_text = \"\\n\".join([f'Review: \"{e[\"text\"]}\"\\nJSON: {{\"predicted_stars\": {e[\"label\"]}}}' for e in examples])\n",
                "    return f\"\"\"\n",
                "    Examples:\n",
                "    {ex_text}\n",
                "    \n",
                "    Classify:\n",
                "    Review: \"{text}\"\n",
                "    Output JSON: {{\"predicted_stars\": <int>, \"explanation\": \"<str>\"}}\n",
                "    \"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Evaluation Loop\n",
                "Iterating through samples and querying the model with all 3 prompts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "No results found. Run the evaluation script.\n"
                    ]
                }
            ],
            "source": [
                "# (See run_evaluation.py for full execution logic to save API costs in interactive mode)\n",
                "# Here we load the results if pre-computed\n",
                "if os.path.exists(\"evaluation_results.csv\"):\n",
                "    df = pd.read_csv(\"evaluation_results.csv\")\n",
                "    print(\"Loaded pre-computed results.\")\n",
                "    print(df.head())\n",
                "else:\n",
                "    print(\"No results found. Run the evaluation script.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Results & Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'df' in locals():\n",
                "    # Accuracy Calculation\n",
                "    acc1 = accuracy_score(df[df['p1_valid']]['actual_stars'], df[df['p1_valid']]['p1_pred'])\n",
                "    acc2 = accuracy_score(df[df['p2_valid']]['actual_stars'], df[df['p2_valid']]['p2_pred'])\n",
                "    acc3 = accuracy_score(df[df['p3_valid']]['actual_stars'], df[df['p3_valid']]['p3_pred'])\n",
                "    \n",
                "    print(f\"Zero-Shot Accuracy: {acc1:.2f}\")\n",
                "    print(f\"Reasoning Accuracy: {acc2:.2f}\")\n",
                "    print(f\"Few-Shot Accuracy:  {acc3:.2f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
